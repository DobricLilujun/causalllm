{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"resource/IF/InfoBench/InFoBench.parquet\")\n",
    "# This is the information for the final evaluation.\n",
    "SYS_MSG = \"\"\"Based on the provided Input (if any) and Generated Text, answer the ensuing Questions with either a YES or NO choice. Your selection should be based on your judgment as well as the following rules:\\n\\n- YES: Select 'YES' if the generated text entirely fulfills the condition specified in the question. However, note that even minor inaccuracies exclude the text from receiving a 'YES' rating. As an illustration. consider a question that asks. \\\"Does each sentence in the generated text use a second person?‚Äù If even one sentence does not use the second person, the answer should NOT be 'YES'. To qualify for a 'YES' rating, the generated text must be entirely accurate and relevant to the question\\n\\n- NO: Opt for 'NO' if the generated text fails to meet the question's requirements or provides no information that could be utilized to answer the question. For instance, if the question asks. \\\"Is the second sentence in the generated text a compound sentence?\\\" and the generated text only has one sentence. it offers no relevant information to answer the question. Consequently, the answer should be 'NO'.'''\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a conscientious assistant, and you must answer my questions exactly as my questions require.\"\n",
    "initial_prompt = \"\"\"\n",
    "Instruction: {system_message}\n",
    "\n",
    "Input: \n",
    "{INSTRUCTION}\n",
    "{INPUT}\n",
    "\"\"\"\n",
    "\n",
    "mixtral_instruct_initial_prompt = \"\"\"<s>[INST]\n",
    "{system_message}\n",
    "{INSTRUCTION}\n",
    "{INPUT}\n",
    "[/INST] \n",
    "</s>\"\"\"\n",
    "\n",
    "llama2_chat_initial_prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "{INSTRUCTION}\n",
    "{INPUT}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "llama3_chat_init_prompt = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "{system_message}\n",
    "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "{INSTRUCTION}\n",
    "{INPUT}\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# instaiate the template prompt\n",
    "initial_prompt_template = PromptTemplate.from_template(initial_prompt)\n",
    "llama2_chat_initial_prompt_template = PromptTemplate.from_template(\n",
    "    llama2_chat_initial_prompt\n",
    ")\n",
    "mixtral_instruct_initial_prompt_template = PromptTemplate.from_template(\n",
    "    mixtral_instruct_initial_prompt\n",
    ")\n",
    "llama3_chat_initial_prompt_template = PromptTemplate.from_template(\n",
    "    llama3_chat_init_prompt\n",
    ")\n",
    "\n",
    "\n",
    "def generate_initial_prompts(row):\n",
    "    input = row[\"input\"]\n",
    "    instruction = row[\"instruction\"]\n",
    "    initial_prompt = initial_prompt_template.format(\n",
    "        INSTRUCTION=instruction, INPUT=input, system_message=system_message\n",
    "    )\n",
    "\n",
    "    llama2_chat_initial_prompt = llama2_chat_initial_prompt_template.format(\n",
    "        INSTRUCTION=instruction, INPUT=input, system_message=system_message\n",
    "    )\n",
    "\n",
    "    llama3_chat_initial_prompt = llama3_chat_initial_prompt_template.format(\n",
    "        INSTRUCTION=instruction, INPUT=input, system_message=system_message\n",
    "    )\n",
    "\n",
    "    mixtral_instruct_initial_prompt = mixtral_instruct_initial_prompt_template.format(\n",
    "        INSTRUCTION=instruction, INPUT=input, system_message=system_message\n",
    "    )\n",
    "\n",
    "    row[\"initial_prompt\"] = initial_prompt\n",
    "    row[\"llama2_chat_initial_prompt\"] = llama2_chat_initial_prompt\n",
    "    row[\"mixtral_instruct_initial_prompt\"] = mixtral_instruct_initial_prompt\n",
    "    row[\"llama3_chat_initial_prompt\"] = llama3_chat_initial_prompt\n",
    "    row[\"INSTRUCTION\"] = input\n",
    "    row[\"INPUT\"] = instruction\n",
    "    return row\n",
    "\n",
    "\n",
    "train_df = train_df.apply(generate_initial_prompts, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "temperatures = np.arange(0.1, 2.0, 0.3)\n",
    "temperature_df = pd.DataFrame({\"Temperature\": temperatures})\n",
    "merged_df = temperature_df.merge(train_df, how=\"cross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"resource/IF/IF_experiment_prompt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
