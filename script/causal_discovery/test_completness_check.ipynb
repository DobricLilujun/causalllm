{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal_discovery_experiment_prompt.csv 的长度为: 19180\n",
      "exp_result_Llama-2-13b-chat-hf_20240506005530.csv 的长度为: 14114\n",
      "exp_result_Meta-Llama-3-8B-Instruct_20240506005718.csv 的长度为: 8528\n",
      "exp_result_Mistral-7B-Instruct-v0.2_20240506014939.csv 的长度为: 14551\n",
      "exp_result_Meta-Llama-3-8B-Instruct_20240507183022.csv 的长度为: 10652\n",
      "exp_result_Llama-2-13b-chat-hf_20240505194445.csv 的长度为: 13\n",
      "exp_result_Llama-2-7b-chat-hf_20240505194019.csv 的长度为: 12\n",
      "exp_result_Llama-2-7b-chat-hf_20240506004401.csv 的长度为: 19180\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# let's check the completeness of the model output\n",
    "import os\n",
    "\n",
    "folder_path = \"resource/input/causal_discovery\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        length = len(df)\n",
    "        print(f\"{filename} 的长度为: {length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14114"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_complete = pd.read_csv(\n",
    "    \"/Users/lujun.li/projects/causallm-github/causalllm/resource/input/causal_discovery/exp_result_Llama-2-7b-chat-hf_20240506004401.csv\"\n",
    ")\n",
    "\n",
    "result_complete1 = pd.read_csv(\n",
    "    \"/Users/lujun.li/projects/causallm-github/causalllm/resource/input/causal_discovery/exp_result_Meta-Llama-3-8B-Instruct_20240506005718.csv\"\n",
    ")\n",
    "\n",
    "result_complete2 = pd.read_csv(\n",
    "    \"/Users/lujun.li/projects/causallm-github/causalllm/resource/input/causal_discovery/exp_result_Meta-Llama-3-8B-Instruct_20240507183022.csv\"\n",
    ")\n",
    "\n",
    "result_complete = pd.concat([result_complete1, result_complete2], ignore_index=True)\n",
    "\n",
    "result_complete = pd.read_csv(\n",
    "    \"/Users/lujun.li/projects/causallm-github/causalllm/resource/input/causal_discovery/exp_result_Llama-2-13b-chat-hf_20240506005530.csv\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "len(result_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1408: '\\n<s>[INST] <<SYS>>\\nYou are a conscientious assistant, and you must answer my questions exactly as my questions require.\\n<</SYS>>\\nPlease reference the following three examples (including premise, query, possible answers, and correct answer) to answer the final query.\\n\\n    Examples: \\n\\n    Premise: A boy strikes a match.\\n    Query: What would have happened if the boy had ditched the match?\\n    Possible answers: The lighting of the match would not have occurred.\\n    Correct answer: He would have been disqualified.\\n    Premise: A boy strikes a match.\\nQuery: What would have happened if the boy had ditched the match?\\nPossible answers: The lighting of the match would not have occurred.He would have been disqualified.The game would have ended tied.\\nCorrect answer:\\n[/INST]\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_complete[\"llama2_chat_initial_prompt\"].sample(1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_response(row):\n",
    "    prompt = row[\"llama2_chat_initial_prompt\"]\n",
    "    response = row[\"generated_response\"]\n",
    "    return response[len(prompt) :]\n",
    "\n",
    "\n",
    "result_complete[\"generated_response_pure\"] = result_complete.apply(\n",
    "    extract_final_response, axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8797: 'Based on the given examples, to answer the final query, \"What would have happened if the bus had run over the family?\", the correct answer would be:\\n\\n\"The family would have been hurt.\"\\n\\nExplanation:\\n\\nIn the given premise, \"A bus unloads a family.\", the focus is on the unloading of the family from the bus. If the bus had run over the family, it would have caused harm to the family, and the bus would not have successfully unloaded them. Therefore, the correct answer is \"The family would have been hurt.\".'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_complete[\"generated_response_pure\"].sample(1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def find_correct_answer(sentence):\n",
    "    pattern = r\"Correct answer:(.+?)\\.\"\n",
    "    match = re.search(pattern, sentence)\n",
    "    if match:\n",
    "        return match.group(1), True\n",
    "    else:\n",
    "        return \"\", False\n",
    "\n",
    "\n",
    "result_complete[[\"generated_answer\", \"is_answer_found\"]] = result_complete[\n",
    "    \"generated_response_pure\"\n",
    "].apply(lambda x: pd.Series(find_correct_answer(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "5        False\n",
       "         ...  \n",
       "14105    False\n",
       "14107    False\n",
       "14108    False\n",
       "14109    False\n",
       "14112    False\n",
       "Name: generated_response_pure, Length: 9970, dtype: bool"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model confused about the input by asking for the query\n",
    "\n",
    "result_complete[result_complete[\"is_answer_found\"] == False][\n",
    "    \"generated_response_pure\"\n",
    "].str.contains(\"Please provide\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_complete[\"is_confused\"] = result_complete[\n",
    "    result_complete[\"is_answer_found\"] == False\n",
    "][\"generated_response_pure\"].apply(lambda x: True if \"Please provide\" in x else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/w4zkqz352hzf1fs2pc1tqjf88vhly_/T/ipykernel_18477/3585300940.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result_complete[\"is_confused\"].fillna(False, inplace=True)\n",
      "/var/folders/k6/w4zkqz352hzf1fs2pc1tqjf88vhly_/T/ipykernel_18477/3585300940.py:1: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_complete[\"is_confused\"].fillna(False, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "result_complete[\"is_confused\"].fillna(False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of the answer\n",
      "14114\n",
      "length of not detected correct answer\n",
      "9970\n",
      "length of not detected correct answer but without confusion\n",
      "9663\n"
     ]
    }
   ],
   "source": [
    "print(\"length of the answer\")\n",
    "print(\n",
    "    len(\n",
    "        result_complete\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"length of not detected correct answer\")\n",
    "print(\n",
    "    len(\n",
    "        result_complete[\n",
    "            result_complete[\"is_answer_found\"] == False\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"length of not detected correct answer but without confusion\")\n",
    "print(\n",
    "    len(\n",
    "        result_complete[\n",
    "            (result_complete[\"is_confused\"] == False)\n",
    "            & (result_complete[\"is_answer_found\"] == False)\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 1 : found correct answer\n",
    "# label 2 : correct answer not found because they didn't unstand the examples and ask for query\n",
    "# label 3 : didn't understand anything\n",
    "result_complete.loc[result_complete[\"is_answer_found\"] == True, \"label\"] = 1\n",
    "\n",
    "result_complete.loc[\n",
    "    (result_complete[\"is_answer_found\"] == False)\n",
    "    & (result_complete[\"is_confused\"] == True),\n",
    "    \"label\",\n",
    "] = 2\n",
    "\n",
    "result_complete.loc[\n",
    "    (result_complete[\"is_answer_found\"] == False)\n",
    "    & (result_complete[\"is_confused\"] == False),\n",
    "    \"label\",\n",
    "] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exact_match\n",
       "False    13957\n",
       "True       157\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "\n",
    "result_complete[\"CorrectAnswerCleaned\"] = result_complete[\"CorrectAnswer\"].str.replace(\n",
    "    r\"[^\\w\\s]\", \"\", regex=True\n",
    ")\n",
    "\n",
    "result_complete[\"exact_match\"] = (\n",
    "    result_complete[\"generated_answer\"].str.strip()\n",
    "    == result_complete[\"CorrectAnswerCleaned\"]\n",
    ")\n",
    "result_complete[\"exact_match\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_answer': {2074: ''},\n",
       " 'CorrectAnswer': {2074: 'That is not possible.'},\n",
       " 'QCC': {2074: 'What would have happened if a video had watched a man?'},\n",
       " 'Premise': {2074: 'A man watches a video.'},\n",
       " 'llama2_chat_initial_prompt': {2074: '\\n<s>[INST] <<SYS>>\\nYou are a conscientious assistant, and you must answer my questions exactly as my questions require.\\n<</SYS>>\\nPlease reference the following three examples (including premise, query, possible answers, and correct answer) to answer the final query.\\n\\n    Examples: \\n\\n    Premise: A man watches a video.\\n    Query: What would have happened if a video had watched a man?\\n    Possible answers: That is not possible.\\n    Correct answer: Nothing special would have happened.\\n    Premise: A man watches a video.\\nQuery: What would have happened if a video had watched a man?\\nPossible answers: That is not possible.Nothing special would have happened.The video would have been happy.\\nCorrect answer:\\n[/INST]\\n'}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_complete[result_complete[\"Temperature\"] == 0.1][\n",
    "    [\n",
    "        \"generated_answer\",\n",
    "        \"CorrectAnswer\",\n",
    "        \"QCC\",\n",
    "        \"Premise\",\n",
    "        \"llama2_chat_initial_prompt\",\n",
    "    ]\n",
    "].sample(1).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "\n",
    "def levenshtein_similarity(str1, str2):\n",
    "    max_length = max(len(str1), len(str2))\n",
    "    if max_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1.0 - (distance(str1, str2) / max_length)\n",
    "\n",
    "\n",
    "result_complete[\"levenshtein_similarity\"] = result_complete.apply(\n",
    "    lambda row: levenshtein_similarity(\n",
    "        row[\"generated_answer\"].strip(), row[\"CorrectAnswerCleaned\"].strip()\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_answer</th>\n",
       "      <th>CorrectAnswer</th>\n",
       "      <th>QCC</th>\n",
       "      <th>Premise</th>\n",
       "      <th>llama2_chat_initial_prompt</th>\n",
       "      <th>levenshtein_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11268</th>\n",
       "      <td>In case someone finds out, they would get in ...</td>\n",
       "      <td>In case someone finds out they would get in tr...</td>\n",
       "      <td>What would have happened if the politician had...</td>\n",
       "      <td>A politician accepts an invitation.</td>\n",
       "      <td>\\n&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are a conscientious a...</td>\n",
       "      <td>0.986486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        generated_answer  \\\n",
       "11268   In case someone finds out, they would get in ...   \n",
       "\n",
       "                                           CorrectAnswer  \\\n",
       "11268  In case someone finds out they would get in tr...   \n",
       "\n",
       "                                                     QCC  \\\n",
       "11268  What would have happened if the politician had...   \n",
       "\n",
       "                                   Premise  \\\n",
       "11268  A politician accepts an invitation.   \n",
       "\n",
       "                              llama2_chat_initial_prompt  \\\n",
       "11268  \\n<s>[INST] <<SYS>>\\nYou are a conscientious a...   \n",
       "\n",
       "       levenshtein_similarity  \n",
       "11268                0.986486  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_complete[\n",
    "    (result_complete[\"levenshtein_similarity\"] >= 0.95)\n",
    "    & (result_complete[\"levenshtein_similarity\"] < 1)\n",
    "][\n",
    "    [\n",
    "        \"generated_answer\",\n",
    "        \"CorrectAnswer\",\n",
    "        \"QCC\",\n",
    "        \"Premise\",\n",
    "        \"llama2_chat_initial_prompt\",\n",
    "        \"levenshtein_similarity\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_counts = (\n",
    "    result_complete[result_complete[\"exact_match\"] == True].groupby(\"Premise\").size()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011123706957630722"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the total number of correct answers (exact matches)\n",
    "total_correct = result_complete[\"exact_match\"].sum()\n",
    "\n",
    "# Calculate the total number of questions\n",
    "total_questions = len(result_complete)\n",
    "\n",
    "# Calculate the T1 score\n",
    "T1_score = total_correct / total_questions\n",
    "T1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
