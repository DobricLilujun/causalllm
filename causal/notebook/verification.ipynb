{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 19/19 [00:12<00:00,  1.55it/s]\n",
      "The model 'MixtralGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "# text_generation_api.py\n",
    "# LLama2 chat 13b \n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import torch\n",
    "from typing import Optional\n",
    "from transformers import BitsAndBytesConfig\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "load_in_4bit = True\n",
    "model_path = \"/home/llama/models/base_models/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print (device)\n",
    "torch.cuda.set_device(1)  # Set the gpu output\n",
    "\n",
    "app = FastAPI()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "quantize_config = BaseQuantizeConfig(bits=4,group_size=128,damp_percent=0.01,desc_act=False)\n",
    "model = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_path, device_map=device, quantization_config = nf4_config\n",
    "# )\n",
    "\n",
    "dynamic_text_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            model_kwargs=\n",
    "                {\n",
    "                    \"load_in_4bit\": load_in_4bit,\n",
    "                    \"device_map\" : device,\n",
    "                },\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_text_pipeline.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputTextWithParams(BaseModel):\n",
    "    text: str\n",
    "    max_new_tokens: Optional[int] = None\n",
    "    max_length: Optional[int] = None\n",
    "    temperature: Optional[float] = None\n",
    "    top_p: Optional[float] = None\n",
    "    do_sample: Optional[bool] = None\n",
    "    repetition_penalty: Optional[float] = None\n",
    "\n",
    "@app.post(\"/generate-text\")\n",
    "async def generate_text(input_data: InputTextWithParams):\n",
    "    try:\n",
    "        generated_text = dynamic_text_pipeline(input_data.text, max_new_tokens= input_data.max_new_tokens, do_sample=input_data.do_sample, temperature=input_data.temperature, top_p=input_data.top_p)\n",
    "\n",
    "        return {\"result\": generated_text}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error generating text: {str(e)}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Write a story about llamas\n",
      "\n",
      "## A Llama’s Tale\n",
      "\n",
      "Once upon a time, there was a little llama named Llama. He lived on a farm with his family and friends. Every day, Llama would go out into the fields and graze on the lush green grass. He loved the feeling of the soft blades tickling his tongue as he chewed.\n",
      "\n",
      "One day, Llama was out in the field with his friend, Llama 2. They were both munching on some delicious grass when they heard a loud noise. It sounded like a car horn.\n",
      "\n",
      "Llama and Llama 2 looked up and saw a big truck coming down the road. The truck was full of people, and they were all waving and cheering.\n",
      "\n",
      "“What’s going on?” Llama asked Llama 2.\n",
      "\n",
      "“I don’t know,” Llama 2 replied. “But it looks like they’re having a lot of fun.”\n",
      "\n",
      "Just then, the truck came to a stop right in front of the two llamas. The people in the truck jumped out and started to set up a big tent.\n",
      "\n",
      "“What are they doing?” Llama asked Llama 2.\n",
      "\n",
      "“I don’t know,” Llama 2 replied. “But it looks like they’re going to have a party.”\n",
      "\n",
      "Sure enough, the people in the truck started to set up tables and chairs, and they brought out all kinds of food and drinks.\n",
      "\n",
      "Llama and Llama 2 watched as the people in the truck danced and sang and ate and drank. They had so much fun that they didn’t even notice when the sun went down and the moon came up.\n",
      "\n",
      "Finally, the people in the truck got tired and went to bed. Llama and Llama 2 were still wide awake, so they decided to go out and explore.\n",
      "\n",
      "They walked around the farm for a while, and then they came to a big pond. The pond was full of water, and there were all kinds of fish swimming around in it.\n",
      "\n",
      "Llama and Llama 2 watched the fish for a while, and then they decided to go for a swim. They jumped into the pond and started to swim around.\n",
      "\n",
      "The water was cold, but it felt good on their\n",
      "*** Pipeline:\n",
      "Write a story about llamas\n",
      "\n",
      "A fun challenge that we set our 7 year old this week was to write a sentence or two, then he added the next one and so on. It's his first time doing it but I think he did really well! He wrote \"I went swimming with my friends\". We had lots of laughs as there were silly sentences in between too:)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-128g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Write a story about llamas\"\n",
    "system_message = \"You are a story writing assistant\"\n",
    "prompt_template=f'''{prompt}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, TextStreamer\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(1)  # Set the gpu output\n",
    "model_path = \"/home/llama/Personal_Directories/srb/causalllm-main/model/Mixtral-8x7B-v0.1-GPTQ\"\n",
    "quantize_config = BaseQuantizeConfig(bits=4)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "config.quantization_config[\"use_exllama\"] = False\n",
    "config.quantization_config[\"bits\"] = 4\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config = config, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, device_map=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> Write a story about llamas\n",
      "\n",
      "In 1977, in the city of Arica, Chile, a man was walking his two llamas down the street. He was a farmer and he had brought his llamas to the city to sell them. As he was walking, he saw a sign that said “Llama Park.” He was curious, so he decided to go in and see what it was all about.\n",
      "\n",
      "When he went inside, he saw a large park with a bunch of llamas in it. He was surprised to see so many llamas in one place. He was also surprised to see that the park was so well-kept. The park had a lot of trees and a lot of grass. It was a beautiful place.\n",
      "\n",
      "The man was happy to see that there were so many people who loved llamas. He was also happy to see that the park was so well-kept. He knew that the park was a good place for llamas to live.\n",
      "\n",
      "The man decided to stay in the park for a while. He wanted to see how the park was run. He also wanted to see how the llamas were treated.\n",
      "\n",
      "The man was impressed with the way the park was run. He was also impressed with the way the llamas were treated. He was happy to see that the park was a good place for llamas to live.\n",
      "\n",
      "The man decided to stay in the park for a while longer. He wanted to see how the park was run and how the llamas were treated. He was happy to see that the park was a good place for llamas to live.\n",
      "\n",
      "The man decided to stay in the park for a while longer. He wanted to see how the park was run and how the llamas were treated. He was happy to see that the park was a good place for llamas to live.\n",
      "\n",
      "The man decided to stay in the park for a while longer. He wanted to see how the park was run and how the llamas were treated. He was happy to see that the park was a good place for llamas to live.\n",
      "\n",
      "The man decided to stay in the park for a while longer. He wanted to see how the park was run and how the llamas were treated. He was happy to see that the park was a good place for llamas to live.\n",
      "\n",
      "The man decided to stay in the park for a while longer. He wanted to see how the park was run and how the llamas were\n",
      "*** Pipeline:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Inference can also be done using transformers' pipeline\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m*** Pipeline:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     top_p\u001b[39m=\u001b[39m\u001b[39m0.95\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39m40\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     repetition_penalty\u001b[39m=\u001b[39m\u001b[39m1.1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(pipe(prompt_template)[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a story about llamas\"\n",
    "system_message = \"You are a story writing assistant\"\n",
    "prompt_template=f'''{prompt}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"model/Mixtral-8x7B-v0.1-GPTQ\")\n",
    "model.save_pretrained(\"model/Mixtral-8x7B-v0.1-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards:   5%|▌         | 1/19 [00:00<00:17,  1.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# torch.cuda.set_device(1)  # Set the gpu output\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39mfrom_pretrained(model_path)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/verification.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_path, config \u001b[39m=\u001b[39;49m config, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,load_in_4bit\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    562\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3503\u001b[0m         model,\n\u001b[1;32m   3504\u001b[0m         state_dict,\n\u001b[1;32m   3505\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3506\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3507\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3508\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3509\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3510\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3511\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3512\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3513\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3514\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3515\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3516\u001b[0m         hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   3517\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3518\u001b[0m     )\n\u001b[1;32m   3520\u001b[0m \u001b[39m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[39m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/modeling_utils.py:3926\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3924\u001b[0m                     hf_quantizer\u001b[39m.\u001b[39mcreate_quantized_param(model, param, key, \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, state_dict)\n\u001b[1;32m   3925\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 3926\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3927\u001b[0m             model_to_load,\n\u001b[1;32m   3928\u001b[0m             state_dict,\n\u001b[1;32m   3929\u001b[0m             loaded_keys,\n\u001b[1;32m   3930\u001b[0m             start_prefix,\n\u001b[1;32m   3931\u001b[0m             expected_keys,\n\u001b[1;32m   3932\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3933\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3934\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3935\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3936\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3937\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3938\u001b[0m             hf_quantizer\u001b[39m=\u001b[39;49mhf_quantizer,\n\u001b[1;32m   3939\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3940\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3941\u001b[0m             unexpected_keys\u001b[39m=\u001b[39;49munexpected_keys,\n\u001b[1;32m   3942\u001b[0m         )\n\u001b[1;32m   3943\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3944\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/modeling_utils.py:807\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    805\u001b[0m         set_module_tensor_to_device(model, param_name, param_device, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mset_module_kwargs)\n\u001b[1;32m    806\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 807\u001b[0m         hf_quantizer\u001b[39m.\u001b[39;49mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n\u001b[1;32m    808\u001b[0m         \u001b[39m# TODO: consider removing used param_parts from state_dict before return\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[39mreturn\u001b[39;00m error_msgs, offload_index, state_dict_index\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:213\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[0;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[1;32m    210\u001b[0m         new_value \u001b[39m=\u001b[39m new_value\u001b[39m.\u001b[39mT\n\u001b[1;32m    212\u001b[0m     kwargs \u001b[39m=\u001b[39m old_value\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[0;32m--> 213\u001b[0m     new_value \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mParams4bit(new_value, requires_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mto(target_device)\n\u001b[1;32m    215\u001b[0m module\u001b[39m.\u001b[39m_parameters[tensor_name] \u001b[39m=\u001b[39m new_value\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/bitsandbytes/nn/modules.py:313\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39m_parse_to(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    312\u001b[0m \u001b[39mif\u001b[39;00m (device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbnb_quantized):\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quantize(device)\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/bitsandbytes/nn/modules.py:279\u001b[0m, in \u001b[0;36mParams4bit._quantize\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_quantize\u001b[39m(\u001b[39mself\u001b[39m, device):\n\u001b[0;32m--> 279\u001b[0m     w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39;49mcuda(device)\n\u001b[1;32m    280\u001b[0m     w_4bit, quant_state \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mquantize_4bit(\n\u001b[1;32m    281\u001b[0m         w,\n\u001b[1;32m    282\u001b[0m         blocksize\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m         quant_storage\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquant_storage,\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m w_4bit\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, TextStreamer\n",
    "from typing import Optional\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import gather_object\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, TextStreamer\n",
    "import torch\n",
    "\n",
    "accelerator = Accelerator()\n",
    "load_in_4bit = False\n",
    "model_path = \"/home/llama/models/base_models/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# torch.cuda.set_device(1)  # Set the gpu output\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config = config, device_map=\"auto\",load_in_4bit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:02<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# text_generation_api.py\n",
    "# LLama2 chat 13b \n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import torch\n",
    "from typing import Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, TextStreamer\n",
    " \n",
    "model_path = \"/home/llama/Personal_Directories/srb/causalllm-main/model/Llama-2-7b-chat-hf\"\n",
    "load_in_4bit = True\n",
    "\n",
    "app = FastAPI()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, config = config, device_map=\"auto\",load_in_4bit=load_in_4bit)\n",
    "\n",
    "dynamic_text_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer = tokenizer,\n",
    "            device_map = \"auto\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
