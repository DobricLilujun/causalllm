{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad2364893fd45ac90f12fde57254dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f44306afda40cda0932943137aacd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "badbefdbff8b4c15b572996ce49cfa23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ed1635d678469b9287728ea982fd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04e1a75cc2f4fc38d55e357e40bce14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI is ongoing, please use the following address ：http://127.0.1.1:8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['/home/lujun/local/causalllm']\n",
      "INFO:     Uvicorn running on http://127.0.1.1:8889 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [749066] using WatchFiles\n",
      "ERROR:    Error loading ASGI app. Attribute \"app\" not found in module \"__main__\".\n",
      "WARNING:  WatchFiles detected changes in 'causal/text_generation_api.py'. Reloading...\n",
      "ERROR:    Error loading ASGI app. Attribute \"app\" not found in module \"__main__\".\n",
      "WARNING:  WatchFiles detected changes in 'causal/text_generation_api.py'. Reloading...\n",
      "ERROR:    Error loading ASGI app. Attribute \"app\" not found in module \"__main__\".\n",
      "WARNING:  WatchFiles detected changes in 'causal/main.py'. Reloading...\n",
      "ERROR:    Error loading ASGI app. Attribute \"app\" not found in module \"__main__\".\n",
      "WARNING:  WatchFiles detected changes in 'causal/main.py'. Reloading...\n",
      "ERROR:    Error loading ASGI app. Attribute \"app\" not found in module \"__main__\".\n",
      "WARNING:  WatchFiles detected changes in 'causal/main.py'. Reloading...\n",
      "ERROR:    Error loading ASGI app. Attribute \"app\" not found in module \"__main__\".\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "# Connection\n",
    "def start_text_generation_api(api_params: dict, model_name: str, model_params: dict):\n",
    "    app = FastAPI(**api_params)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, trust_remote_code=True, device_map=device\n",
    "    )\n",
    "    generation_config = GenerationConfig.from_pretrained(model_name, **model_params)\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "    class InputText(BaseModel):\n",
    "        text: str\n",
    "\n",
    "    @app.post(\"/generate-text\")\n",
    "    def generate_text(input_text: InputText):\n",
    "        try:\n",
    "            generated_text = text_pipeline(input_text.text, max_length=250)[0][\n",
    "                \"generated_text\"\n",
    "            ]\n",
    "            return {\"result\": generated_text}\n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=500, detail=f\"Error generating text: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    uvicorn_cmd = f\"uvicorn {__name__}:app --host {api_params['host']} --port {api_params['port']} --reload\"\n",
    "    print(\n",
    "        f\"FastAPI is ongoing, please use the following address ：http://{api_params['host']}:{api_params['port']}\"\n",
    "    )\n",
    "    os.system(uvicorn_cmd)\n",
    "\n",
    "\n",
    "api_params = {\n",
    "    \"host\": \"127.0.1.1\",\n",
    "    \"port\": 8889,\n",
    "}\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model_params = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.0001,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "}\n",
    "\n",
    "start_text_generation_api(api_params, model_name, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "input_text = {\"text\": \"Your input is here for the verification.\"}\n",
    "response = requests.post(\"http://127.0.1.1:8889/generate-text\", json=input_text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()[\"result\"]\n",
    "    print(f\"Generated Text: {result}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
