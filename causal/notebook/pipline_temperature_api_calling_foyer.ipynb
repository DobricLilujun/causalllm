{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipline Causal Analysis - Temperature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from utils.ollamaUtil import generate_response\n",
    "from utils.evaluate import calculate_cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation - Take only 300 for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset_path = \"resource/data/databricks-dolly-15k\"\n",
    "dataset = load_from_disk(local_dataset_path)\n",
    "df = dataset.to_pandas()\n",
    "test_df = df[df[\"category\"] == \"closed_qa\"]\n",
    "test = test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(input_string):\n",
    "    start_tag = \"<Answer>\"\n",
    "    end_tag = \"</Answer>\"\n",
    "\n",
    "    if start_tag in input_string and end_tag in input_string:\n",
    "        return input_string.split(start_tag, 1)[1].split(end_tag, 1)[0]\n",
    "    else:\n",
    "        return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 GPU(s).\n",
      "GPU 0: Allocated Memory = 0.00 GB, Max Memory = 79.11 GB\n",
      "GPU 1: Allocated Memory = 0.00 GB, Max Memory = 79.11 GB\n",
      "GPU 2: Allocated Memory = 0.00 GB, Max Memory = 79.11 GB\n",
      "GPU 3: Allocated Memory = 0.00 GB, Max Memory = 79.11 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    " \n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    " \n",
    "if num_gpus > 0:\n",
    "\n",
    "    print(f\"Found {num_gpus} GPU(s).\")\n",
    "\n",
    "    for gpu_index in range(num_gpus):\n",
    "\n",
    "        torch.cuda.set_device(gpu_index)\n",
    "\n",
    "        allocated_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "        max_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "\n",
    "        print(f\"GPU {gpu_index}: Allocated Memory = {allocated_memory / 1024**3:.2f} GB, Max Memory = {max_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.\n403 Client Error. (Request ID: Root=1-66001c03-708bfbd71cff2dcd30be0a58;f6305e8f-9874-4282-8e76-1163277932a3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-hf to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    399\u001b[0m         path_or_repo_id,\n\u001b[1;32m    400\u001b[0m         filename,\n\u001b[1;32m    401\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    402\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    403\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    404\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    405\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    406\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    407\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    408\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    409\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    410\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1403\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1402\u001b[0m     \u001b[39m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1405\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1262\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1263\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1264\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1265\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1266\u001b[0m         library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[1;32m   1267\u001b[0m         library_version\u001b[39m=\u001b[39;49mlibrary_version,\n\u001b[1;32m   1268\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[1;32m   1270\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1667\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1668\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1669\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1670\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1671\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1672\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1673\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1674\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1675\u001b[0m )\n\u001b[1;32m   1676\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    386\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    387\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    388\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    389\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/utils/_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    318\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot access gated repo for url \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0m     \u001b[39mraise\u001b[39;00m GatedRepoError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39melif\u001b[39;00m error_message \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAccess to this resource is disabled.\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-66001c03-708bfbd71cff2dcd30be0a58;f6305e8f-9874-4282-8e76-1163277932a3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-hf to ask for access.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m model_params \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmax_new_tokens\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1024\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0.0001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrepetition_penalty\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1.15\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m }\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m start_text_generation_api(api_params, model_name, model_params)\n",
      "\u001b[1;32m/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m app \u001b[39m=\u001b[39m FastAPI(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mapi_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, use_fast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     model_name, torch_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat16, trust_remote_code\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, device_map\u001b[39m=\u001b[39mdevice\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m generation_config \u001b[39m=\u001b[39m GenerationConfig\u001b[39m.\u001b[39mfrom_pretrained(model_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_params)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:782\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m config_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m--> 782\u001b[0m         config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    783\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    784\u001b[0m         )\n\u001b[1;32m    785\u001b[0m     config_tokenizer_class \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mtokenizer_class\n\u001b[1;32m    786\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(config, \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoTokenizer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1111\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1109\u001b[0m code_revision \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1111\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1112\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1113\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    632\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    635\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/configuration_utils.py:688\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    686\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    689\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    690\u001b[0m         configuration_file,\n\u001b[1;32m    691\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    692\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    693\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    694\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    695\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    696\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    697\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    698\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    699\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    700\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    702\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    703\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/utils/hub.py:416\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m resolved_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    415\u001b[0m         \u001b[39mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to have access to it at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.\n403 Client Error. (Request ID: Root=1-66001c03-708bfbd71cff2dcd30be0a58;f6305e8f-9874-4282-8e76-1163277932a3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-hf to ask for access."
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import torch\n",
    "import os\n",
    " \n",
    " \n",
    "def start_text_generation_api(api_params: dict, model_name: str, model_params: dict):\n",
    "    app = FastAPI(**api_params)\n",
    " \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, trust_remote_code=True, device_map=device\n",
    "    )\n",
    "    generation_config = GenerationConfig.from_pretrained(model_name, **model_params)\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    " \n",
    "    class InputText(BaseModel):\n",
    "        text: str\n",
    " \n",
    "    @app.post(\"/generate-text\")\n",
    "    def generate_text(input_text: InputText):\n",
    "        try:\n",
    "            generated_text = text_pipeline(input_text.text, max_length=250)[0][\n",
    "                \"generated_text\"\n",
    "            ]\n",
    "            return {\"result\": generated_text}\n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=500, detail=f\"Error generating text: {str(e)}\"\n",
    "            )\n",
    " \n",
    "    uvicorn_cmd = f\"uvicorn {__name__}:app --host {api_params['host']} --port {api_params['port']} --reload\"\n",
    "    print(\n",
    "        f\"FastAPI is ongoing, please use the following address : http://{api_params['host']}:{api_params['port']}\"\n",
    "    )\n",
    "    os.system(uvicorn_cmd)\n",
    " \n",
    " \n",
    "api_params = {\n",
    "    \"host\": \"127.0.0.2\",\n",
    "    \"port\": 8889,\n",
    "}\n",
    " \n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    " \n",
    "model_params = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.0001,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "}\n",
    " \n",
    "start_text_generation_api(api_params, model_name, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    " \n",
    "input_text = {\"text\": \"Your input is here for the verification.\"}\n",
    "response = requests.post(\"http://127.0.0.1:8000/generate-text\", json=input_text)\n",
    " \n",
    "if response.status_code == 200:\n",
    "    result = response.json()[\"result\"]\n",
    "    print(f\"Generated Text: {result}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import bfloat16\n",
    "\n",
    "\n",
    "TOKEN = \"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\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.cuda.set_device(2)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Downloading shards: 100%|██████████| 3/3 [05:10<00:00, 103.54s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import bfloat16\n",
    "\n",
    "TOKEN = \"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\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "torch.cuda.set_device(3)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, use_auth_token=TOKEN\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type='nf4',\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_compute_dtype=bfloat16\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    use_auth_token=TOKEN,\n",
    ")\n",
    " \n",
    "# Generate\n",
    "# inputs = tokenizer(\"Hello world\", return_tensors=\"pt\").to(\"cuda\")\n",
    "# generated_ids = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_p=0.95,\n",
    "#     temperature=0.01,\n",
    "#     max_length=250,\n",
    "# )\n",
    "# tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    " \n",
    "### Option 2\n",
    "# Replace 'XXX' with the model variant of your choosing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n403 Client Error. (Request ID: Root=1-66001d83-301bb39339a6f7900bbcdbce;ecfb9637-286b-4c15-9318-60483dafd140)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-chat-hf to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    399\u001b[0m         path_or_repo_id,\n\u001b[1;32m    400\u001b[0m         filename,\n\u001b[1;32m    401\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    402\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    403\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    404\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    405\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    406\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    407\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    408\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    409\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    410\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1403\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1402\u001b[0m     \u001b[39m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1405\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1262\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1263\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1264\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1265\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1266\u001b[0m         library_name\u001b[39m=\u001b[39;49mlibrary_name,\n\u001b[1;32m   1267\u001b[0m         library_version\u001b[39m=\u001b[39;49mlibrary_version,\n\u001b[1;32m   1268\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[1;32m   1270\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:1667\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1668\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1669\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1670\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m   1671\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1672\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1673\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1674\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1675\u001b[0m )\n\u001b[1;32m   1676\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    386\u001b[0m         method\u001b[39m=\u001b[39;49mmethod,\n\u001b[1;32m    387\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    388\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    389\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams,\n\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    392\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/huggingface_hub/utils/_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    318\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot access gated repo for url \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0m     \u001b[39mraise\u001b[39;00m GatedRepoError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[39melif\u001b[39;00m error_message \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAccess to this resource is disabled.\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 403 Client Error. (Request ID: Root=1-66001d83-301bb39339a6f7900bbcdbce;ecfb9637-286b-4c15-9318-60483dafd140)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-chat-hf to ask for access.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# model_kwargs={\"load_in_8bit\": True}, # quantize to 8-bit\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# model_kwargs={\"load_in_4bit\": True}, # quantize to 4-bit\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# use_auth_token=TOKEN,\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Generate\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m pipe(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHello world\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m250\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsfpl-ai-01/home/llama/Personal_Directories/srb/causalllm-main/causal/notebook/pipline_temperature_api_calling_foyer.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/pipelines/__init__.py:815\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m                 adapter_config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m    813\u001b[0m                 model \u001b[39m=\u001b[39m adapter_config[\u001b[39m\"\u001b[39m\u001b[39mbase_model_name_or_path\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m--> 815\u001b[0m     config \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    816\u001b[0m         model, _from_pipeline\u001b[39m=\u001b[39;49mtask, code_revision\u001b[39m=\u001b[39;49mcode_revision, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs\n\u001b[1;32m    817\u001b[0m     )\n\u001b[1;32m    818\u001b[0m     hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39m_commit_hash\n\u001b[1;32m    820\u001b[0m custom_tasks \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1111\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   1109\u001b[0m code_revision \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mcode_revision\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1111\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1112\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1113\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/configuration_utils.py:633\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    632\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    635\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/configuration_utils.py:688\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    686\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    687\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    689\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    690\u001b[0m         configuration_file,\n\u001b[1;32m    691\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    692\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    693\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    694\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    695\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    696\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    697\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    698\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    699\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    700\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    701\u001b[0m     )\n\u001b[1;32m    702\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    703\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/utils/hub.py:416\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[39mif\u001b[39;00m resolved_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    415\u001b[0m         \u001b[39mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    417\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to have access to it at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    425\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`token=<your_token>`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    426\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n403 Client Error. (Request ID: Root=1-66001d83-301bb39339a6f7900bbcdbce;ecfb9637-286b-4c15-9318-60483dafd140)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-chat-hf is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-2-7b-chat-hf to ask for access."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    # model_kwargs={\"load_in_8bit\": True}, # quantize to 8-bit\n",
    "    # model_kwargs={\"load_in_4bit\": True}, # quantize to 4-bit\n",
    "    # use_auth_token=TOKEN,\n",
    ")\n",
    " \n",
    "# Generate\n",
    "pipe(\n",
    "    \"Hello world\",\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    max_length=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"model/Llama-2-13b-chat-hf\")\n",
    "model.save_pretrained(\"model/Llama-2-13b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 61/61 [00:39<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained(\"model/Llama-2-7b-hf\")\n",
    "# tokenizer.save_pretrained(\"model/Llama-2-7b-hf\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/llama/Personal_Directories/srb/causalllm-main/model/Llama-2-70b-chat-hf\") \n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/llama/Personal_Directories/srb/causalllm-main/model/Llama-2-70b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello world!\\n\\nThis is a test message.\\n\\nPlease ignore.\\n\\nThank you.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    " \n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model= model,\n",
    "    tokenizer = tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\"load_in_8bit\": True}, # quantize to 8-bit\n",
    "    model_kwargs={\"load_in_4bit\": True}, # quantize to 4-bit\n",
    "    # use_auth_token=TOKEN,\n",
    ")\n",
    " \n",
    "pipe(\n",
    "    \"Hello world\",\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    max_length=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "instruction = \"\"\"Instruction: Please strictly answer the question, but format it using custom tags like <Answer>Your answer here<Answer>. If the question cannot be answered, just answer with \"I don't know\". \"\"\"\n",
    "template = \"\"\"\n",
    "Instruction: {instruction}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "columns = [\n",
    "    \"temperature\",\n",
    "    \"model\",\n",
    "    \"model_tag\",\n",
    "    \"question\",\n",
    "    \"context\",\n",
    "    \"ground_truth_response\",\n",
    "    \"prompt\",\n",
    "    \"payload\",\n",
    "]\n",
    "result_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the preprocessed csv files\n",
    "for index, row in test.iterrows():\n",
    "    question = row[\"instruction\"]\n",
    "    context = row[\"context\"]\n",
    "    response = row[\"response\"]\n",
    "    category = row[\"category\"]\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        instruction=instruction, context=context, question=question\n",
    "    )\n",
    "\n",
    "    for model in [\"7b\"]:\n",
    "        for temperature in np.arange(0.1, 1.1, 0.3):\n",
    "            model_tag = \"llama2:\" + model\n",
    "            data = {\n",
    "                \"temperature\": temperature,\n",
    "                \"model\": \"llama\" + model,\n",
    "                \"model_tag\": model_tag,\n",
    "                \"question\": question,\n",
    "                \"context\": context,\n",
    "                \"ground_truth_response\": response,\n",
    "                \"prompt\": str(prompt),\n",
    "            }\n",
    "            result_df = pd.concat([result_df, pd.DataFrame([data])], ignore_index=True)\n",
    "\n",
    "\n",
    "result_df.to_excel(\"job_2_temperature_newtest_new.xlsx\")\n",
    "loaded_df = pd.read_excel(\"job_2_temperature_newtest_new.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiments\n",
    "csv_filename = \"job_2_temperature_newtest_new.csv\"\n",
    "pbar = tqdm(total=len(result_df))\n",
    "responses = []\n",
    "similarities = []\n",
    "for index, row in result_df.iterrows():\n",
    "    response = remove_tags(generate_response(row))\n",
    "    similarity = calculate_cosine_similarity(response, row[\"ground_truth_response\"])\n",
    "    updated_row = row.copy()\n",
    "    updated_row[\"generated_response\"] = response\n",
    "    updated_row[\"cosine_similarity\"] = similarity\n",
    "    updated_dataframe = pd.DataFrame([updated_row])\n",
    "    updated_dataframe.to_csv(csv_filename, index=False, mode=\"a\", header=not index)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "result_df = pd.read_csv(\"job_3_temperature_llama2-7b-result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>temperature</th>\n",
       "      <th>model</th>\n",
       "      <th>model_tag</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>ground_truth_response</th>\n",
       "      <th>prompt</th>\n",
       "      <th>payload</th>\n",
       "      <th>generated_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>temperature</td>\n",
       "      <td>model</td>\n",
       "      <td>model_tag</td>\n",
       "      <td>question</td>\n",
       "      <td>context</td>\n",
       "      <td>ground_truth_response</td>\n",
       "      <td>prompt</td>\n",
       "      <td>payload</td>\n",
       "      <td>generated_response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>1769</td>\n",
       "      <td>1.6</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Which animal is associated with Chengdu?</td>\n",
       "      <td>\"Chengtu, is a sub-provincial city which serve...</td>\n",
       "      <td>Chengdu is associated with the giant panda, a ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>1770</td>\n",
       "      <td>1.9</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Which animal is associated with Chengdu?</td>\n",
       "      <td>\"Chengtu, is a sub-provincial city which serve...</td>\n",
       "      <td>Chengdu is associated with the giant panda, a ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>1771</td>\n",
       "      <td>0.1</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Given this paragraph about hockey what are dif...</td>\n",
       "      <td>Hockey is a term used to denote a family of va...</td>\n",
       "      <td>Hockey can refer to Ice Hockey, Field Hockey, ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>1772</td>\n",
       "      <td>0.4</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Given this paragraph about hockey what are dif...</td>\n",
       "      <td>Hockey is a term used to denote a family of va...</td>\n",
       "      <td>Hockey can refer to Ice Hockey, Field Hockey, ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>1773</td>\n",
       "      <td>0.7</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Given this paragraph about hockey what are dif...</td>\n",
       "      <td>Hockey is a term used to denote a family of va...</td>\n",
       "      <td>Hockey can refer to Ice Hockey, Field Hockey, ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3878 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  temperature     model   model_tag  \\\n",
       "0              0          0.1  llama13b  llama2:13b   \n",
       "1              1          0.4  llama13b  llama2:13b   \n",
       "2     Unnamed: 0  temperature     model   model_tag   \n",
       "3              0          0.1  llama13b  llama2:13b   \n",
       "4              1          0.4  llama13b  llama2:13b   \n",
       "...          ...          ...       ...         ...   \n",
       "3873        1769          1.6  llama13b  llama2:13b   \n",
       "3874        1770          1.9  llama13b  llama2:13b   \n",
       "3875        1771          0.1  llama13b  llama2:13b   \n",
       "3876        1772          0.4  llama13b  llama2:13b   \n",
       "3877        1773          0.7  llama13b  llama2:13b   \n",
       "\n",
       "                                               question  \\\n",
       "0            When did Virgin Australia start operating?   \n",
       "1            When did Virgin Australia start operating?   \n",
       "2                                              question   \n",
       "3            When did Virgin Australia start operating?   \n",
       "4            When did Virgin Australia start operating?   \n",
       "...                                                 ...   \n",
       "3873           Which animal is associated with Chengdu?   \n",
       "3874           Which animal is associated with Chengdu?   \n",
       "3875  Given this paragraph about hockey what are dif...   \n",
       "3876  Given this paragraph about hockey what are dif...   \n",
       "3877  Given this paragraph about hockey what are dif...   \n",
       "\n",
       "                                                context  \\\n",
       "0     Virgin Australia, the trading name of Virgin A...   \n",
       "1     Virgin Australia, the trading name of Virgin A...   \n",
       "2                                               context   \n",
       "3     Virgin Australia, the trading name of Virgin A...   \n",
       "4     Virgin Australia, the trading name of Virgin A...   \n",
       "...                                                 ...   \n",
       "3873  \"Chengtu, is a sub-provincial city which serve...   \n",
       "3874  \"Chengtu, is a sub-provincial city which serve...   \n",
       "3875  Hockey is a term used to denote a family of va...   \n",
       "3876  Hockey is a term used to denote a family of va...   \n",
       "3877  Hockey is a term used to denote a family of va...   \n",
       "\n",
       "                                  ground_truth_response  \\\n",
       "0     Virgin Australia commenced services on 31 Augu...   \n",
       "1     Virgin Australia commenced services on 31 Augu...   \n",
       "2                                 ground_truth_response   \n",
       "3     Virgin Australia commenced services on 31 Augu...   \n",
       "4     Virgin Australia commenced services on 31 Augu...   \n",
       "...                                                 ...   \n",
       "3873  Chengdu is associated with the giant panda, a ...   \n",
       "3874  Chengdu is associated with the giant panda, a ...   \n",
       "3875  Hockey can refer to Ice Hockey, Field Hockey, ...   \n",
       "3876  Hockey can refer to Ice Hockey, Field Hockey, ...   \n",
       "3877  Hockey can refer to Ice Hockey, Field Hockey, ...   \n",
       "\n",
       "                                                 prompt  payload  \\\n",
       "0     \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "1     \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "2                                                prompt  payload   \n",
       "3     \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "4     \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "...                                                 ...      ...   \n",
       "3873  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "3874  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "3875  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "3876  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "3877  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "\n",
       "                                     generated_response  \n",
       "0     \\nInstruction: Please strictly answer the ques...  \n",
       "1     \\nInstruction: Please strictly answer the ques...  \n",
       "2                                    generated_response  \n",
       "3                                                   NaN  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "3873  \\nInstruction: Please strictly answer the ques...  \n",
       "3874  \\nInstruction: Please strictly answer the ques...  \n",
       "3875  \\nInstruction: Please strictly answer the ques...  \n",
       "3876  \\nInstruction: Please strictly answer the ques...  \n",
       "3877  \\nInstruction: Please strictly answer the ques...  \n",
       "\n",
       "[3878 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The follwoing is ruuning: \n",
      "PID: 1129745, Name: uvicorn, Command: /usr/bin/python3 /home/llama/.local/bin/uvicorn causal.text_generation_api:app --reload --host 127.0.0.1 --port 8899\n",
      "PID: 2371149, Name: uvicorn, Command: /home/llama/Sandbox_IA/env/bin/python /home/llama/Sandbox_IA/env/bin/uvicorn app.main:app --reload --port 8001\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import subprocess\n",
    "import signal\n",
    " \n",
    "def find_uvicorn_processes():\n",
    "    uvicorn_processes = []\n",
    "    for process in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        if 'uvicorn' in process.info['name'].lower() or \\\n",
    "                ('uvicorn' in process.info['cmdline'] if process.info['cmdline'] else False):\n",
    "            uvicorn_processes.append(process.info)\n",
    " \n",
    "    return uvicorn_processes\n",
    " \n",
    "\n",
    "def kill_process_by_pid(pid):\n",
    "    try:\n",
    "        process = psutil.Process(pid)\n",
    "        process.terminate()  # Alternatively, use process.kill() for a forceful termination\n",
    "        print(f\"Process {pid} has been terminated.\")\n",
    "    except psutil.NoSuchProcess:\n",
    "        print(f\"No process found with PID {pid}.\")\n",
    "\n",
    "\n",
    "uvicorn_processes = find_uvicorn_processes()\n",
    "\n",
    "if uvicorn_processes:\n",
    "    print(\"The follwoing is ruuning: \")\n",
    "    for process in uvicorn_processes:\n",
    "        print(f\"PID: {process['pid']}, Name: {process['name']}, Command: {' '.join(process['cmdline'])}\")\n",
    "else:\n",
    "    print(\"Uvicore unfound\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
