{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uvicorn causal.text_generation_api:app --reload --host 127.0.0.1 --port 8889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from utils.evaluate import calculate_cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import subprocess\n",
    "import signal\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the api is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The follwoing is ruuning: \n",
      "PID: 24290, Name: uvicorn, Command: /home/llama/Sandbox_IA/env/bin/python /home/llama/Sandbox_IA/env/bin/uvicorn app.main:app --reload --port 8001\n",
      "PID: 1261240, Name: python, Command: /home/llama/Personal_Directories/srb/causalEnv/bin/python -m uvicorn causal.text_generation_api_llama2_7B:app --reload --host 127.0.0.1 --port 8899\n"
     ]
    }
   ],
   "source": [
    "def find_uvicorn_processes():\n",
    "    uvicorn_processes = []\n",
    "    for process in psutil.process_iter(['pid', 'name', 'cmdline']):\n",
    "        if 'uvicorn' in process.info['name'].lower() or \\\n",
    "                ('uvicorn' in process.info['cmdline'] if process.info['cmdline'] else False):\n",
    "            uvicorn_processes.append(process.info)\n",
    " \n",
    "    return uvicorn_processes\n",
    " \n",
    "def kill_process_by_pid(pid):\n",
    "    try:\n",
    "        process = psutil.Process(pid)\n",
    "        process.terminate()  # Alternatively, use process.kill() for a forceful termination\n",
    "        print(f\"Process {pid} has been terminated.\")\n",
    "    except psutil.NoSuchProcess:\n",
    "        print(f\"No process found with PID {pid}.\")\n",
    "\n",
    "uvicorn_processes = find_uvicorn_processes()\n",
    "\n",
    "if uvicorn_processes:\n",
    "    print(\"The follwoing is ruuning: \")\n",
    "    for process in uvicorn_processes:\n",
    "        print(f\"PID: {process['pid']}, Name: {process['name']}, Command: {' '.join(process['cmdline'])}\")\n",
    "else:\n",
    "    print(\"Uvicore unfound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the server using uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"causal\"\n",
    "module = \"text_generation_api_llama2_13B\"\n",
    "model = \"LLama_13B\"\n",
    "host = \"127.0.0.1\"\n",
    "port = 8890\n",
    "quantization = \"4bit\"\n",
    "python_executable = \"~/Personal_Directories/srb/causalEnv/bin/python\"\n",
    "test_num = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: '~/Personal_Directories/srb/causalEnv/bin/pyt...>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['/home/llama/Personal_Directories/srb/causalllm-main']\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8890 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [1262030] using StatReload\n",
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib64/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/uvicorn/_subprocess.py\", line 78, in subprocess_started\n",
      "    target(sockets=sockets)\n",
      "  File \"/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/uvicorn/server.py\", line 65, in run\n",
      "    return asyncio.run(self.serve(sockets=sockets))\n",
      "  File \"/usr/lib64/python3.9/asyncio/runners.py\", line 44, in run\n",
      "    return loop.run_until_complete(main)\n",
      "  File \"/usr/lib64/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/uvicorn/server.py\", line 69, in serve\n",
      "    await self._serve(sockets)\n",
      "  File \"/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/uvicorn/server.py\", line 76, in _serve\n",
      "    config.load()\n",
      "  File \"/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/uvicorn/config.py\", line 433, in load\n",
      "    self.loaded_app = import_from_string(self.app)\n",
      "  File \"/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/uvicorn/importer.py\", line 19, in import_from_string\n",
      "    module = importlib.import_module(module_str)\n",
      "  File \"/usr/lib64/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/llama/Personal_Directories/srb/causalllm-main/causal/text_generation_api_llama2_13B.py\", line 14, in <module>\n",
      "    config = AutoConfig.from_pretrained(model_path)\n",
      "NameError: name 'AutoConfig' is not defined\n",
      "WARNING:  StatReload detected changes in 'causal/text_generation_api_llama2_13B.py'. Reloading...\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards:   9%|▉         | 1/11 [00:01<00:13,  1.39s/it]WARNING:  StatReload detected changes in 'causal/text_generation_api_llama2_13B.py'. Reloading...\n",
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:14<00:00,  1.28s/it]\n",
      "INFO:     Started server process [1262188]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    " \n",
    "# %nohup run_uvicorn_command(directory, your_module, host,port)\n",
    "uvicorn_command = f\"{python_executable} -m uvicorn {directory}.{module}:app --reload --host {host} --port {port}\"\n",
    "subprocess.Popen(uvicorn_command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(input_string):\n",
    "    match = re.search(r'<Answer>(.*?)<\\/Answer>', input_string)\n",
    "    if match:\n",
    "        answer = match.group(1)\n",
    "        return answer, True\n",
    "    else:\n",
    "        return input_string, False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"max_colwidth\", 1000)\n",
    "\n",
    "local_dataset_path = \"resource/data/databricks-dolly-15k\"\n",
    "dataset = load_from_disk(local_dataset_path)\n",
    "df = dataset.to_pandas()\n",
    "test_df = df[df[\"category\"] == \"closed_qa\"]\n",
    "test = test_df.head(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1261086/1750806520.py:47: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df = pd.concat([result_df, pd.DataFrame([data])], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "url = \"http://127.0.0.1:8899/generate-text\"\n",
    "system_prompt = \"\"\"\n",
    "Please strictly answer my question, but format it using custom tags like <Answer>Your answer here<Answer>. If the question cannot be answered, just answer with \"I don't know\". \"\"\"\n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "Context: {context_message}\n",
    "Question: {question_message}\n",
    "\n",
    "[/INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# template = \"\"\"\n",
    "# Instruction:{system_prompt}\n",
    "# Context: {context_message}\n",
    "# Question: {question_message}\n",
    "# \"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "columns = [\n",
    "    \"temperature\",\n",
    "    \"model\",\n",
    "    \"ground_truth_response\",\n",
    "    \"prompt\"\n",
    "]\n",
    "result_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# build the preprocessed csv files\n",
    "for index, row in test.iterrows():\n",
    "    question = row[\"instruction\"]\n",
    "    context = row[\"context\"]\n",
    "    response = row[\"response\"]\n",
    "    prompt = prompt_template.format(\n",
    "        system_prompt=system_prompt, context_message=context, question_message = question\n",
    "    )\n",
    "\n",
    "    for temperature in np.arange(0.1, 2.0, 0.3):\n",
    "        data = {\n",
    "            \"temperature\": temperature,\n",
    "            \"model\": model,\n",
    "            \"ground_truth_response\": response,\n",
    "            \"prompt\": str(prompt),\n",
    "        }\n",
    "        result_df = pd.concat([result_df, pd.DataFrame([data])], ignore_index=True)\n",
    "\n",
    "\n",
    "result_df.to_excel(f\"{module}_{quantization}_{test_num}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_excel(f\"{module}_{quantization}_{test_num}.xlsx\")\n",
    "result_df[\"temperature\"] = result_df[\"temperature\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.huggingfaceUtil import generate_response_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2100 [00:00<?, ?it/s]/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/bitsandbytes/nn/modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2100 [00:14<8:35:57, 14.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52494 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2100 [00:16<3:59:36,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:32872 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2100 [00:17<2:27:22,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:32878 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2100 [00:18<1:43:56,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:32892 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2100 [00:19<1:19:56,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:32894 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2100 [00:20<1:08:31,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:32910 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2100 [00:21<1:00:45,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:32912 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2100 [00:23<53:34,  1.54s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37186 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2100 [00:24<48:49,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37192 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/2100 [00:25<45:26,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37206 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llama/Personal_Directories/srb/causalEnv/lib64/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "  1%|          | 11/2100 [00:26<43:11,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37214 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/2100 [00:27<42:23,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37228 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 13/2100 [00:28<41:01,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37234 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/2100 [00:30<46:02,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37240 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 15/2100 [00:36<1:40:06,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37250 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 16/2100 [00:43<2:17:47,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:59130 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 17/2100 [00:51<3:01:56,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37616 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 18/2100 [00:58<3:24:50,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:37974 - \"POST /generate-text HTTP/1.1\" 200 OK\n",
      "1.3\n"
     ]
    }
   ],
   "source": [
    "from utils.huggingfaceUtil import generate_response_hf\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time  # Import the time module\n",
    " \n",
    "csv_filename = f\"{module}_{quantization}_{test_num}.csv\"\n",
    "\n",
    "pbar = tqdm(total=len(result_df))\n",
    "\n",
    "responses = []\n",
    "\n",
    "similarities = []\n",
    " \n",
    "for index, row in result_df.iterrows():\n",
    "    start_time = time.time()  # Record the start time\n",
    "    response = generate_response_hf(row)\n",
    "    end_time = time.time()  # Record the end time\n",
    "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
    "    updated_row = row.copy()\n",
    "    response_str = response[0][\"generated_text\"]\n",
    "    updated_row[\"full answer\"] = response_str\n",
    "    answer, label = extract_answer(response_str)\n",
    "    updated_row[\"generated_response\"] = answer\n",
    "    updated_row[\"label\"] = label\n",
    "    updated_row[\"elapsed_time\"] = elapsed_time  # Add elapsed time to the row\n",
    "    # Add the timestamp to the updated_row\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    updated_row[\"timestamp\"] = timestamp\n",
    "    updated_dataframe = pd.DataFrame([updated_row])\n",
    "    updated_dataframe.to_csv(csv_filename, index=False, mode=\"a\", header=not index)\n",
    "    pbar.update(1)\n",
    "    \n",
    "pbar.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
