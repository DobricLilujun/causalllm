{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipline Causal Analysis - Temperature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q datasets\n",
    "! pip install -q ydata-profiling\n",
    "! pip install -q transformers\n",
    "! pip install -q --upgrade openai\n",
    "! pip install -q huggingface_hub\n",
    "! pip install scikit-learn torch langchain\n",
    "! ollama pull llama2:7b\n",
    "! ollama pull llama2:13b\n",
    "# ! ollama pull llama2:7b-chat-q4_0\n",
    "# ! ollama pull llama2:7b-chat-q5_0\n",
    "# ! ollama pull llama2:7b-chat-q8_0\n",
    "# ! ollama pull llama2:7b-chat-fp16\n",
    "# ! ollama pull llama2:13b-chat-q4_0\n",
    "# ! ollama pull llama2:13b-chat-q5_0\n",
    "# ! ollama pull llama2:13b-chat-q8_0\n",
    "# ! ollama pull llama2:13b-chat-fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain import PromptTemplate\n",
    "from utils.ollamaUtil import generate_response\n",
    "from utils.evaluate import calculate_cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation - Take only 300 for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset_path = \"resource/data/databricks-dolly-15k\"\n",
    "dataset = load_from_disk(local_dataset_path)\n",
    "df = dataset.to_pandas()\n",
    "test_df = df[df[\"category\"] == \"closed_qa\"]\n",
    "test = test_df.head(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(input_string):\n",
    "    start_tag = \"<Answer>\"\n",
    "    end_tag = \"</Answer>\"\n",
    "\n",
    "    if start_tag in input_string and end_tag in input_string:\n",
    "        return input_string.split(start_tag, 1)[1].split(end_tag, 1)[0]\n",
    "    else:\n",
    "        return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "instruction = \"\"\"Instruction: Please strictly answer the question, but format it using custom tags like <Answer>Your answer here<Answer>. If the question cannot be answered, just answer with \"I don't know\". \"\"\"\n",
    "template = \"\"\"\n",
    "Instruction: {instruction}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "columns = [\n",
    "    \"temperature\",\n",
    "    \"model\",\n",
    "    \"model_tag\",\n",
    "    \"question\",\n",
    "    \"context\",\n",
    "    \"ground_truth_response\",\n",
    "    \"prompt\",\n",
    "    \"payload\",\n",
    "]\n",
    "result_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the preprocessed csv files\n",
    "for index, row in test.iterrows():\n",
    "    question = row[\"instruction\"]\n",
    "    context = row[\"context\"]\n",
    "    response = row[\"response\"]\n",
    "    category = row[\"category\"]\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        instruction=instruction, context=context, question=question\n",
    "    )\n",
    "\n",
    "    for model in [\"7b\"]:\n",
    "        for temperature in np.arange(0.1, 1.1, 0.3):\n",
    "            model_tag = \"llama2:\" + model\n",
    "            data = {\n",
    "                \"temperature\": temperature,\n",
    "                \"model\": \"llama\" + model,\n",
    "                \"model_tag\": model_tag,\n",
    "                \"question\": question,\n",
    "                \"context\": context,\n",
    "                \"ground_truth_response\": response,\n",
    "                \"prompt\": str(prompt),\n",
    "            }\n",
    "            result_df = pd.concat([result_df, pd.DataFrame([data])], ignore_index=True)\n",
    "\n",
    "\n",
    "result_df.to_excel(\"job_2_temperature_newtest_new.xlsx\")\n",
    "loaded_df = pd.read_excel(\"job_2_temperature_newtest_new.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiments\n",
    "csv_filename = \"job_2_temperature_newtest_new.csv\"\n",
    "pbar = tqdm(total=len(result_df))\n",
    "responses = []\n",
    "similarities = []\n",
    "for index, row in result_df.iterrows():\n",
    "    response = remove_tags(generate_response(row))\n",
    "    similarity = calculate_cosine_similarity(response, row[\"ground_truth_response\"])\n",
    "    updated_row = row.copy()\n",
    "    updated_row[\"generated_response\"] = response\n",
    "    updated_row[\"cosine_similarity\"] = similarity\n",
    "    updated_dataframe = pd.DataFrame([updated_row])\n",
    "    updated_dataframe.to_csv(csv_filename, index=False, mode=\"a\", header=not index)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
