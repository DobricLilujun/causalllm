{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipline Causal Analysis - Temperature 4bit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/causalLLM/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# from datasets import load_from_disk\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from langchain import PromptTemplate\n",
    "# from utils.ollamaUtil import generate_response\n",
    "# from utils.evaluate import calculate_cosine_similarity\n",
    "# from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation - Take only 300 for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dataset_path = \"resource/data/databricks-dolly-15k\"\n",
    "dataset = load_from_disk(local_dataset_path)\n",
    "df = dataset.to_pandas()\n",
    "test_df = df[df[\"category\"] == \"closed_qa\"]\n",
    "test = test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(input_string):\n",
    "    start_tag = \"<Answer>\"\n",
    "    end_tag = \"</Answer>\"\n",
    "\n",
    "    if start_tag in input_string and end_tag in input_string:\n",
    "        return input_string.split(start_tag, 1)[1].split(end_tag, 1)[0]\n",
    "    else:\n",
    "        return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 GPU(s).\n",
      "GPU 0: Allocated Memory = 0.00 GB, Max Memory = 79.11 GB\n",
      "GPU 1: Allocated Memory = 0.00 GB, Max Memory = 79.11 GB\n",
      "GPU 2: Allocated Memory = 0.00 GB, Max Memory = 79.11 GB\n",
      "GPU 3: Allocated Memory = 0.00 GB, Max Memory = 79.11 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "if num_gpus > 0:\n",
    "\n",
    "    print(f\"Found {num_gpus} GPU(s).\")\n",
    "\n",
    "    for gpu_index in range(num_gpus):\n",
    "\n",
    "        torch.cuda.set_device(gpu_index)\n",
    "\n",
    "        allocated_memory = torch.cuda.memory_allocated()\n",
    "\n",
    "        max_memory = torch.cuda.get_device_properties(gpu_index).total_memory\n",
    "\n",
    "        print(\n",
    "            f\"GPU {gpu_index}: Allocated Memory = {allocated_memory / 1024**3:.2f} GB, Max Memory = {max_memory / 1024**3:.2f} GB\"\n",
    "        )\n",
    "else:\n",
    "    print(\"No GPU available. Using CPU.\")\n",
    "\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastAPI is ongoing, please use the following address : http://127.0.0.2:8889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['/home/llama/Personal_Directories/srb/causalllm-main']\n",
      "INFO:     Uvicorn running on http://127.0.0.2:8889 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [609937] using StatReload\n",
      "ERROR:    Error loading ASGI app. Attribute \"app\" not found in module \"__main__\".\n"
     ]
    }
   ],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "def start_text_generation_api(api_params: dict, model_name: str, model_params: dict):\n",
    "    app = FastAPI(**api_params)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, torch_dtype=torch.float16, trust_remote_code=True, device_map=device\n",
    "    )\n",
    "    generation_config = GenerationConfig.from_pretrained(model_name, **model_params)\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "\n",
    "    class InputText(BaseModel):\n",
    "        text: str\n",
    "\n",
    "    @app.post(\"/generate-text\")\n",
    "    def generate_text(input_text: InputText):\n",
    "        try:\n",
    "            generated_text = text_pipeline(input_text.text, max_length=250)[0][\n",
    "                \"generated_text\"\n",
    "            ]\n",
    "            return {\"result\": generated_text}\n",
    "        except Exception as e:\n",
    "            raise HTTPException(\n",
    "                status_code=500, detail=f\"Error generating text: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    uvicorn_cmd = f\"uvicorn {__name__}:app --host {api_params['host']} --port {api_params['port']} --reload\"\n",
    "    print(\n",
    "        f\"FastAPI is ongoing, please use the following address : http://{api_params['host']}:{api_params['port']}\"\n",
    "    )\n",
    "    os.system(uvicorn_cmd)\n",
    "\n",
    "\n",
    "api_params = {\n",
    "    \"host\": \"127.0.0.2\",\n",
    "    \"port\": 8889,\n",
    "}\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model_params = {\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"temperature\": 0.0001,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "    \"repetition_penalty\": 1.15,\n",
    "}\n",
    "\n",
    "start_text_generation_api(api_params, model_name, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "input_text = {\"text\": \"Your input is here for the verification.\"}\n",
    "response = requests.post(\"http://127.0.0.1:8000/generate-text\", json=input_text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()[\"result\"]\n",
    "    print(f\"Generated Text: {result}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n\u001b[1;32m      5\u001b[0m TOKEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_TxbrbngkQSYdxHKXzfkftqiRWRMspkKMyL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import bfloat16\n",
    "\n",
    "\n",
    "TOKEN = \"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\"\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch.cuda.set_device(2)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bfloat16\n\u001b[1;32m      4\u001b[0m TOKEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_TxbrbngkQSYdxHKXzfkftqiRWRMspkKMyL\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import bfloat16\n",
    "\n",
    "TOKEN = \"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\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.set_device(3)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-70b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=TOKEN)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=bnb_config,\n",
    "    # trust_remote_code=True,\n",
    "    device_map=device,\n",
    "    use_auth_token=TOKEN,\n",
    ")\n",
    "\n",
    "# Generate\n",
    "# inputs = tokenizer(\"Hello world\", return_tensors=\"pt\").to(\"cuda\")\n",
    "# generated_ids = model.generate(\n",
    "#     **inputs,\n",
    "#     do_sample=True,\n",
    "#     top_p=0.95,\n",
    "#     temperature=0.01,\n",
    "#     max_length=250,\n",
    "# )\n",
    "# tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "### Option 2\n",
    "# Replace 'XXX' with the model variant of your choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.95s/it]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello world!\\n\\nThis is a test message.\\n\\nPlease ignore.\\n\\nThank you.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    device_map=\"auto\",\n",
    "    # model_kwargs={\"load_in_8bit\": True}, # quantize to 8-bit\n",
    "    # model_kwargs={\"load_in_4bit\": True}, # quantize to 4-bit\n",
    "    # use_auth_token=TOKEN,\n",
    ")\n",
    "\n",
    "# Generate\n",
    "pipe(\n",
    "    \"Hello world\",\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    max_length=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"model/Llama-2-70b-chat-hf\")\n",
    "model.save_pretrained(\"model/Llama-2-70b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 61/61 [00:39<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# model.save_pretrained(\"model/Llama-2-7b-hf\")\n",
    "# tokenizer.save_pretrained(\"model/Llama-2-7b-hf\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/home/llama/Personal_Directories/srb/causalllm-main/model/Llama-2-70b-chat-hf\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/home/llama/Personal_Directories/srb/causalllm-main/model/Llama-2-70b-chat-hf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello world!\\n\\nThis is a test message.\\n\\nPlease ignore.\\n\\nThank you.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\"load_in_8bit\": True},  # quantize to 8-bit\n",
    "    model_kwargs={\"load_in_4bit\": True},  # quantize to 4-bit\n",
    "    # use_auth_token=TOKEN,\n",
    ")\n",
    "\n",
    "pipe(\n",
    "    \"Hello world\",\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    temperature=0.01,\n",
    "    max_length=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/generate\"\n",
    "instruction = \"\"\"Instruction: Please strictly answer the question, but format it using custom tags like <Answer>Your answer here<Answer>. If the question cannot be answered, just answer with \"I don't know\". \"\"\"\n",
    "template = \"\"\"\n",
    "Instruction: {instruction}\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "columns = [\n",
    "    \"temperature\",\n",
    "    \"model\",\n",
    "    \"model_tag\",\n",
    "    \"question\",\n",
    "    \"context\",\n",
    "    \"ground_truth_response\",\n",
    "    \"prompt\",\n",
    "    \"payload\",\n",
    "]\n",
    "result_df = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the preprocessed csv files\n",
    "for index, row in test.iterrows():\n",
    "    question = row[\"instruction\"]\n",
    "    context = row[\"context\"]\n",
    "    response = row[\"response\"]\n",
    "    category = row[\"category\"]\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        instruction=instruction, context=context, question=question\n",
    "    )\n",
    "\n",
    "    for model in [\"7b\"]:\n",
    "        for temperature in np.arange(0.1, 1.1, 0.3):\n",
    "            model_tag = \"llama2:\" + model\n",
    "            data = {\n",
    "                \"temperature\": temperature,\n",
    "                \"model\": \"llama\" + model,\n",
    "                \"model_tag\": model_tag,\n",
    "                \"question\": question,\n",
    "                \"context\": context,\n",
    "                \"ground_truth_response\": response,\n",
    "                \"prompt\": str(prompt),\n",
    "            }\n",
    "            result_df = pd.concat([result_df, pd.DataFrame([data])], ignore_index=True)\n",
    "\n",
    "\n",
    "result_df.to_excel(\"job_2_temperature_newtest_new.xlsx\")\n",
    "loaded_df = pd.read_excel(\"job_2_temperature_newtest_new.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiments\n",
    "csv_filename = \"job_2_temperature_newtest_new.csv\"\n",
    "pbar = tqdm(total=len(result_df))\n",
    "responses = []\n",
    "similarities = []\n",
    "for index, row in result_df.iterrows():\n",
    "    response = remove_tags(generate_response(row))\n",
    "    similarity = calculate_cosine_similarity(response, row[\"ground_truth_response\"])\n",
    "    updated_row = row.copy()\n",
    "    updated_row[\"generated_response\"] = response\n",
    "    updated_row[\"cosine_similarity\"] = similarity\n",
    "    updated_dataframe = pd.DataFrame([updated_row])\n",
    "    updated_dataframe.to_csv(csv_filename, index=False, mode=\"a\", header=not index)\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.read_csv(\"job_3_temperature_llama2-7b-result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>temperature</th>\n",
       "      <th>model</th>\n",
       "      <th>model_tag</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>ground_truth_response</th>\n",
       "      <th>prompt</th>\n",
       "      <th>payload</th>\n",
       "      <th>generated_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Unnamed: 0</td>\n",
       "      <td>temperature</td>\n",
       "      <td>model</td>\n",
       "      <td>model_tag</td>\n",
       "      <td>question</td>\n",
       "      <td>context</td>\n",
       "      <td>ground_truth_response</td>\n",
       "      <td>prompt</td>\n",
       "      <td>payload</td>\n",
       "      <td>generated_response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>When did Virgin Australia start operating?</td>\n",
       "      <td>Virgin Australia, the trading name of Virgin A...</td>\n",
       "      <td>Virgin Australia commenced services on 31 Augu...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3873</th>\n",
       "      <td>1769</td>\n",
       "      <td>1.6</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Which animal is associated with Chengdu?</td>\n",
       "      <td>\"Chengtu, is a sub-provincial city which serve...</td>\n",
       "      <td>Chengdu is associated with the giant panda, a ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>1770</td>\n",
       "      <td>1.9</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Which animal is associated with Chengdu?</td>\n",
       "      <td>\"Chengtu, is a sub-provincial city which serve...</td>\n",
       "      <td>Chengdu is associated with the giant panda, a ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>1771</td>\n",
       "      <td>0.1</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Given this paragraph about hockey what are dif...</td>\n",
       "      <td>Hockey is a term used to denote a family of va...</td>\n",
       "      <td>Hockey can refer to Ice Hockey, Field Hockey, ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>1772</td>\n",
       "      <td>0.4</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Given this paragraph about hockey what are dif...</td>\n",
       "      <td>Hockey is a term used to denote a family of va...</td>\n",
       "      <td>Hockey can refer to Ice Hockey, Field Hockey, ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3877</th>\n",
       "      <td>1773</td>\n",
       "      <td>0.7</td>\n",
       "      <td>llama13b</td>\n",
       "      <td>llama2:13b</td>\n",
       "      <td>Given this paragraph about hockey what are dif...</td>\n",
       "      <td>Hockey is a term used to denote a family of va...</td>\n",
       "      <td>Hockey can refer to Ice Hockey, Field Hockey, ...</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nInstruction: Please strictly answer the ques...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3878 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  temperature     model   model_tag  \\\n",
       "0              0          0.1  llama13b  llama2:13b   \n",
       "1              1          0.4  llama13b  llama2:13b   \n",
       "2     Unnamed: 0  temperature     model   model_tag   \n",
       "3              0          0.1  llama13b  llama2:13b   \n",
       "4              1          0.4  llama13b  llama2:13b   \n",
       "...          ...          ...       ...         ...   \n",
       "3873        1769          1.6  llama13b  llama2:13b   \n",
       "3874        1770          1.9  llama13b  llama2:13b   \n",
       "3875        1771          0.1  llama13b  llama2:13b   \n",
       "3876        1772          0.4  llama13b  llama2:13b   \n",
       "3877        1773          0.7  llama13b  llama2:13b   \n",
       "\n",
       "                                               question  \\\n",
       "0            When did Virgin Australia start operating?   \n",
       "1            When did Virgin Australia start operating?   \n",
       "2                                              question   \n",
       "3            When did Virgin Australia start operating?   \n",
       "4            When did Virgin Australia start operating?   \n",
       "...                                                 ...   \n",
       "3873           Which animal is associated with Chengdu?   \n",
       "3874           Which animal is associated with Chengdu?   \n",
       "3875  Given this paragraph about hockey what are dif...   \n",
       "3876  Given this paragraph about hockey what are dif...   \n",
       "3877  Given this paragraph about hockey what are dif...   \n",
       "\n",
       "                                                context  \\\n",
       "0     Virgin Australia, the trading name of Virgin A...   \n",
       "1     Virgin Australia, the trading name of Virgin A...   \n",
       "2                                               context   \n",
       "3     Virgin Australia, the trading name of Virgin A...   \n",
       "4     Virgin Australia, the trading name of Virgin A...   \n",
       "...                                                 ...   \n",
       "3873  \"Chengtu, is a sub-provincial city which serve...   \n",
       "3874  \"Chengtu, is a sub-provincial city which serve...   \n",
       "3875  Hockey is a term used to denote a family of va...   \n",
       "3876  Hockey is a term used to denote a family of va...   \n",
       "3877  Hockey is a term used to denote a family of va...   \n",
       "\n",
       "                                  ground_truth_response  \\\n",
       "0     Virgin Australia commenced services on 31 Augu...   \n",
       "1     Virgin Australia commenced services on 31 Augu...   \n",
       "2                                 ground_truth_response   \n",
       "3     Virgin Australia commenced services on 31 Augu...   \n",
       "4     Virgin Australia commenced services on 31 Augu...   \n",
       "...                                                 ...   \n",
       "3873  Chengdu is associated with the giant panda, a ...   \n",
       "3874  Chengdu is associated with the giant panda, a ...   \n",
       "3875  Hockey can refer to Ice Hockey, Field Hockey, ...   \n",
       "3876  Hockey can refer to Ice Hockey, Field Hockey, ...   \n",
       "3877  Hockey can refer to Ice Hockey, Field Hockey, ...   \n",
       "\n",
       "                                                 prompt  payload  \\\n",
       "0     \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "1     \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "2                                                prompt  payload   \n",
       "3     \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "4     \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "...                                                 ...      ...   \n",
       "3873  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "3874  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "3875  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "3876  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "3877  \\nInstruction: Please strictly answer the ques...      NaN   \n",
       "\n",
       "                                     generated_response  \n",
       "0     \\nInstruction: Please strictly answer the ques...  \n",
       "1     \\nInstruction: Please strictly answer the ques...  \n",
       "2                                    generated_response  \n",
       "3                                                   NaN  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "3873  \\nInstruction: Please strictly answer the ques...  \n",
       "3874  \\nInstruction: Please strictly answer the ques...  \n",
       "3875  \\nInstruction: Please strictly answer the ques...  \n",
       "3876  \\nInstruction: Please strictly answer the ques...  \n",
       "3877  \\nInstruction: Please strictly answer the ques...  \n",
       "\n",
       "[3878 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The follwoing is ruuning: \n",
      "PID: 1129745, Name: uvicorn, Command: /usr/bin/python3 /home/llama/.local/bin/uvicorn causal.text_generation_api:app --reload --host 127.0.0.1 --port 8899\n",
      "PID: 2371149, Name: uvicorn, Command: /home/llama/Sandbox_IA/env/bin/python /home/llama/Sandbox_IA/env/bin/uvicorn app.main:app --reload --port 8001\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import subprocess\n",
    "import signal\n",
    "\n",
    "\n",
    "def find_uvicorn_processes():\n",
    "    uvicorn_processes = []\n",
    "    for process in psutil.process_iter([\"pid\", \"name\", \"cmdline\"]):\n",
    "        if \"uvicorn\" in process.info[\"name\"].lower() or (\n",
    "            \"uvicorn\" in process.info[\"cmdline\"] if process.info[\"cmdline\"] else False\n",
    "        ):\n",
    "            uvicorn_processes.append(process.info)\n",
    "\n",
    "    return uvicorn_processes\n",
    "\n",
    "\n",
    "def kill_process_by_pid(pid):\n",
    "    try:\n",
    "        process = psutil.Process(pid)\n",
    "        process.terminate()  # Alternatively, use process.kill() for a forceful termination\n",
    "        print(f\"Process {pid} has been terminated.\")\n",
    "    except psutil.NoSuchProcess:\n",
    "        print(f\"No process found with PID {pid}.\")\n",
    "\n",
    "\n",
    "uvicorn_processes = find_uvicorn_processes()\n",
    "\n",
    "if uvicorn_processes:\n",
    "    print(\"The follwoing is ruuning: \")\n",
    "    for process in uvicorn_processes:\n",
    "        print(\n",
    "            f\"PID: {process['pid']}, Name: {process['name']}, Command: {' '.join(process['cmdline'])}\"\n",
    "        )\n",
    "else:\n",
    "    print(\"Uvicore unfound\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causalLLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
